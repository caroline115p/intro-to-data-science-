{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Classification with Support Vector Machines\n",
    "\n",
    "# Total: 11 Marks \n",
    "## Instructions\n",
    "\n",
    "* Complete the assignment\n",
    "* Once the notebook is complete, restart your kernel and rerun your cells\n",
    "* Submit this notebook to owl by the deadline\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "This dataset consists of 3921 e-mails to a single account, some of which are spam. These data represent incoming emails for the first three months of 2012 for an email account.\n",
    "\n",
    "The table has 3921 (1252) observations on the following 21 variables.\n",
    "\n",
    "* spam: Indicator for whether the email was spam.\n",
    "* to_multiple: Indicator for whether the email was addressed to more than one recipient.\n",
    "* from: Whether the message was listed as from anyone (this is usually set by default for regular outgoing email).\n",
    "* cc: Indicator for whether anyone was CCed.\n",
    "* sent_email: Indicator for whether the sender had been sent an email in the last 30 days.\n",
    "* time: Time at which email was sent.\n",
    "* image: The number of images attached.\n",
    "* attach: The number of attached files.\n",
    "* dollar: The number of times a dollar sign or the word “dollar” appeared in the email.\n",
    "* winner: Indicates whether “winner” appeared in the email.\n",
    "* inherit: The number of times “inherit” (or an extension, such as “inheritance”) appeared in the email.\n",
    "* viagra: The number of times “viagra” appeared in the email.\n",
    "* password: The number of times “password” appeared in the email.\n",
    "* num_char: The number of characters in the email, in thousands.\n",
    "* line_breaks: The number of line breaks in the email (does not count text wrapping).\n",
    "* format: Indicates whether the email was written using HTML (e.g. may have included bolding or active links).\n",
    "* re_subj: Whether the subject started with “Re:”, “RE:”, “re:”, or “rE:”\n",
    "* exclaim_subj: Whether there was an exclamation point in the subject.\n",
    "* urgent_subj: Whether the word “urgent” was in the email subject.\n",
    "* exclaim_mess: The number of exclamation points in the email message.\n",
    "* number: Factor variable saying whether there was no number, a small number (under 1 million), or a big number.\n",
    "\n",
    "The data are from this R package: https://cran.r-project.org/web/packages/openintro/openintro.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need these\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score, make_scorer, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: 1 Mark\n",
    "\n",
    "Read in the `email.txt` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('email.txt', delimiter='\\t')\n",
    "\n",
    "df = df.drop('time', axis = 'columns')\n",
    "df = df.drop('number',axis = 'columns')\n",
    "df = df.drop('winner',axis = 'columns')\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: 1 Mark\n",
    "\n",
    "Split the data into train and test.  Hold out 50% of observations as the test set.  Pass `random_state=0` to `train_test_split` to ensure you get the same train and tests sets as the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('spam', axis='columns').astype('float').values\n",
    "y = df.spam.values\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,\n",
    "                                                y, \n",
    "                                                test_size=0.5, \n",
    "                                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: 1 Mark\n",
    "\n",
    "Create a pipeline for your support vector machine.  You can start with a `kernel=\"linear\"` and `gamma=\"auto\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('SVC', SVC(kernel = 'linear', gamma = 'auto') )\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: 3 Marks\n",
    "\n",
    "Use your model to construct a confusion matrix by fitting and predicting on the training data (I've inlcluded a little helper function to make looking at the confusion matrix a little easier). Then answer the following using the confusion matrix (don't use sklearn's functions):\n",
    "\n",
    "* What is your model's training accuracy?\n",
    "* What is your model's training precision?\n",
    "* What is your model's training recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes=np.array([0.0,1.0]), normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('equal')\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout();\n",
    "    return ax\n",
    "\n",
    "pipe.fit(Xtrain, ytrain)\n",
    "\n",
    "prediction = pipe.predict(Xtrain)\n",
    "\n",
    "plot_confusion_matrix(ytrain, prediction, classes = np.array([0, 1]));\n",
    "\n",
    "matrix = confusion_matrix(ytrain, prediction)\n",
    "\n",
    "diagsum = np.diag(matrix).sum()\n",
    "\n",
    "accuracy = diagsum/matrix.sum()\n",
    "\n",
    "precision = matrix[1,1]/np.sum(matrix[:,1])\n",
    "\n",
    "recall = matrix[1,1]/np.sum(matrix[1,:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E:  1 Mark\n",
    "\n",
    "Estimate your support vector machine's out of sample recall by using 5 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(pipe, Xtrain, ytrain, cv = 5, scoring=make_scorer(recall_score))\n",
    "\n",
    "score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F: 2 Marks\n",
    "\n",
    "  Use sklearn's `GridSearchCV` to search over the kernel and gamma. Search over `kernel = ['rbf','sigmoid']` and `gamma = np.linspace(1e-5, 1e-2)`.  Use recall as your metric for scoring.\n",
    "  \n",
    "\n",
    "`GridSearchCV` is a way to cross validate your models for a variety of parameters.  Read more about `GridSearchCV` [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = Pipeline([\n",
    "    ('SVC', SVC() )\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {'SVC__kernel': ['rbf','sigmoid'],\n",
    "              'SVC__gamma': np.linspace(1e-5, 1e-2)}\n",
    "\n",
    "svc_gscv = GridSearchCV(svc_pipe, \n",
    "                          param_grid, \n",
    "                          cv = 5,\n",
    "                          scoring=make_scorer(recall_score),\n",
    "                          iid = True)\n",
    "\n",
    "svc_gscv.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part G: 1 Mark\n",
    "\n",
    "What was the cross validated recall for your regularized model?  If you called your model grid search `svc_gscv` you can access the best model's score by performing `svc_gscv.best_score_`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gscv.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part H: 1 Mark\n",
    "\n",
    "You can access the results of the cross validation `.cv_results_` method. If you called your estimator `svc_gcsv` then call `svc_gscv.cv_results_`.  This will return a dictionary.  You can turn it into a dataframe using `pandas.DataFrame` (this will make it easier to manipulate).  Learn more about `.cv_results_` [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "Plot how the mean test error changes as `gamma` changes.  Color the lines according to `kernel`. What do you see happening to the cross validated error as gamma increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(svc_gscv.cv_results_)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "\n",
    "sns.lineplot(data = cv_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
